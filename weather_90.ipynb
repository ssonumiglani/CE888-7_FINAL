{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e599f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sweetviz as sv\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.datasets import make_imbalance\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split,KFold, cross_val_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import (confusion_matrix,recall_score, make_scorer,\n",
    "                    precision_score,accuracy_score,f1_score,\n",
    "                    silhouette_score, completeness_score,silhouette_samples)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "679e4a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set on Australia's weather\n",
    "df2 = pd.read_csv('weatherAUS.csv')\n",
    "df2.head()\n",
    "\n",
    "df2.drop(columns=['Evaporation','Evaporation','Cloud9am','Cloud3pm'], inplace=True)\n",
    "df2 = df2[df2.notna()]\n",
    "df2 = df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9a490cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (66646, 21)\n",
      "Class Proportions:\n",
      " No     0.778126\n",
      "Yes    0.221874\n",
      "Name: RainTomorrow, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Before splitting into test and training set, we explore the size and the class proportions in order to confirm it has the right level of imbalance\n",
    "print(\"Shape: \", df2.shape)\n",
    "#Class proportions\n",
    "print(\"Class Proportions:\\n\", df2[\"RainTomorrow\"].value_counts()/df2[\"RainTomorrow\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52bba368",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2.reset_index(inplace=True)\n",
    "df2.drop(columns=['index','Date'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "059f7b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     51859\n",
       "Yes    14787\n",
       "Name: RainTomorrow, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['RainTomorrow'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e5a1c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    43993\n",
      "1    12427\n",
      "Name: RainTomorrow, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Train-Test split into 90-10\n",
    "X_2=df2.iloc[:,0:-1].copy()\n",
    "Y_2=df2.iloc[:,-1].copy()\n",
    "y_map = {'No':'0', 'Yes':'1'}\n",
    "Y_2 = Y_2.map(y_map)\n",
    "print(y_2.value_counts())\n",
    "\n",
    "x_2=X_2.copy()\n",
    "y_2=Y_2.copy()\n",
    "\n",
    "# x2_train, x2_test, y2_train, y2_test=train_test_split(x_2,y_2, train_size=0.9, random_state=21, stratify=y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4987c265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RISK_MM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>17.9</td>\n",
       "      <td>35.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>SSW</td>\n",
       "      <td>48.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>1004.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>33.4</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>18.4</td>\n",
       "      <td>28.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>S</td>\n",
       "      <td>37.0</td>\n",
       "      <td>SSE</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1012.9</td>\n",
       "      <td>1012.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-04</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>19.4</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>10.6</td>\n",
       "      <td>NNE</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1012.3</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.7</td>\n",
       "      <td>34.9</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>21.9</td>\n",
       "      <td>38.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>12.2</td>\n",
       "      <td>WNW</td>\n",
       "      <td>31.0</td>\n",
       "      <td>WNW</td>\n",
       "      <td>...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1012.7</td>\n",
       "      <td>1009.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29.1</td>\n",
       "      <td>35.6</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>24.2</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>WNW</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1010.7</td>\n",
       "      <td>1007.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>37.6</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56415</th>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>Darwin</td>\n",
       "      <td>19.3</td>\n",
       "      <td>33.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>35.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>63.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1013.9</td>\n",
       "      <td>1010.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56416</th>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>Darwin</td>\n",
       "      <td>21.2</td>\n",
       "      <td>32.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>8.6</td>\n",
       "      <td>E</td>\n",
       "      <td>37.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1014.6</td>\n",
       "      <td>1011.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.8</td>\n",
       "      <td>32.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56417</th>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>Darwin</td>\n",
       "      <td>20.7</td>\n",
       "      <td>32.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>E</td>\n",
       "      <td>33.0</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1015.3</td>\n",
       "      <td>1011.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.8</td>\n",
       "      <td>32.1</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56418</th>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>Darwin</td>\n",
       "      <td>19.5</td>\n",
       "      <td>31.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>ESE</td>\n",
       "      <td>26.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1014.9</td>\n",
       "      <td>1010.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.8</td>\n",
       "      <td>29.2</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56419</th>\n",
       "      <td>2017-06-24</td>\n",
       "      <td>Darwin</td>\n",
       "      <td>20.2</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>10.7</td>\n",
       "      <td>ENE</td>\n",
       "      <td>30.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1013.9</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56420 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0      2009-01-01    Cobar     17.9     35.2       0.0         12.0      12.3   \n",
       "1      2009-01-02    Cobar     18.4     28.9       0.0         14.8      13.0   \n",
       "2      2009-01-04    Cobar     19.4     37.6       0.0         10.8      10.6   \n",
       "3      2009-01-05    Cobar     21.9     38.4       0.0         11.4      12.2   \n",
       "4      2009-01-06    Cobar     24.2     41.0       0.0         11.2       8.4   \n",
       "...           ...      ...      ...      ...       ...          ...       ...   \n",
       "56415  2017-06-20   Darwin     19.3     33.4       0.0          6.0      11.0   \n",
       "56416  2017-06-21   Darwin     21.2     32.6       0.0          7.6       8.6   \n",
       "56417  2017-06-22   Darwin     20.7     32.8       0.0          5.6      11.0   \n",
       "56418  2017-06-23   Darwin     19.5     31.8       0.0          6.2      10.6   \n",
       "56419  2017-06-24   Darwin     20.2     31.7       0.0          5.6      10.7   \n",
       "\n",
       "      WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
       "0             SSW           48.0        ENE  ...        20.0         13.0   \n",
       "1               S           37.0        SSE  ...        30.0          8.0   \n",
       "2             NNE           46.0        NNE  ...        42.0         22.0   \n",
       "3             WNW           31.0        WNW  ...        37.0         22.0   \n",
       "4             WNW           35.0         NW  ...        19.0         15.0   \n",
       "...           ...            ...        ...  ...         ...          ...   \n",
       "56415         ENE           35.0         SE  ...        63.0         32.0   \n",
       "56416           E           37.0         SE  ...        56.0         28.0   \n",
       "56417           E           33.0          E  ...        46.0         23.0   \n",
       "56418         ESE           26.0         SE  ...        62.0         58.0   \n",
       "56419         ENE           30.0        ENE  ...        73.0         32.0   \n",
       "\n",
       "       Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  \\\n",
       "0           1006.3       1004.4       2.0       5.0     26.6     33.4   \n",
       "1           1012.9       1012.1       1.0       1.0     20.3     27.0   \n",
       "2           1012.3       1009.2       1.0       6.0     28.7     34.9   \n",
       "3           1012.7       1009.1       1.0       5.0     29.1     35.6   \n",
       "4           1010.7       1007.4       1.0       6.0     33.6     37.6   \n",
       "...            ...          ...       ...       ...      ...      ...   \n",
       "56415       1013.9       1010.5       0.0       1.0     24.5     32.3   \n",
       "56416       1014.6       1011.2       7.0       0.0     24.8     32.0   \n",
       "56417       1015.3       1011.8       0.0       0.0     24.8     32.1   \n",
       "56418       1014.9       1010.7       1.0       1.0     24.8     29.2   \n",
       "56419       1013.9       1009.7       6.0       5.0     25.4     31.0   \n",
       "\n",
       "       RainToday  RISK_MM  \n",
       "0             No      0.0  \n",
       "1             No      0.0  \n",
       "2             No      0.0  \n",
       "3             No      0.0  \n",
       "4             No      0.0  \n",
       "...          ...      ...  \n",
       "56415         No      0.0  \n",
       "56416         No      0.0  \n",
       "56417         No      0.0  \n",
       "56418         No      0.0  \n",
       "56419         No      0.0  \n",
       "\n",
       "[56420 rows x 23 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5711b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalanced_90_X, imbalanced_90_y = make_imbalance(X_2,Y_2, sampling_strategy={'0':39539,'1':4800})\n",
    "imbalanced_90_y = imbalanced_90_y.astype(int)\n",
    "x2_train, x2_test, y2_train, y2_test=train_test_split(imbalanced_90_X,imbalanced_90_y, train_size=0.90, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5cf55f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:  (39905, 19)\n",
      "Train class Proportions:\n",
      " 0    0.890791\n",
      "1    0.109209\n",
      "Name: RainTomorrow, dtype: float64\n",
      "Test shape:  (4434, 19)\n",
      "Test class Proportions:\n",
      " 0    0.900316\n",
      "1    0.099684\n",
      "Name: RainTomorrow, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#We check the proportions of the sets to confirm the imbalance is preserved\n",
    "print(\"Train Shape: \", x2_train.shape)\n",
    "#Class proportions\n",
    "print(\"Train class Proportions:\\n\", y2_train.value_counts()/y2_train.shape)\n",
    "\n",
    "print(\"Test shape: \", x2_test.shape)\n",
    "#Class proportions\n",
    "print(\"Test class Proportions:\\n\", y2_test.value_counts()/y2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2201009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstraping mean\n",
    "def boostrap_mean(sample, sample_size, n_iterations):\n",
    "\tbootstrap_samples=np.random.choice(sample, size=(n_iterations,sample_size), replace=True)\n",
    "\tmean_array=np.mean(bootstrap_samples, axis=1)\n",
    "\tdata_mean=np.mean(mean_array)\n",
    "\tlower=np.percentile(mean_array,2.5)\n",
    "\tupper=np.percentile(mean_array,97.5)\n",
    "\treturn data_mean, lower, upper\n",
    "\n",
    "#Bootstraping standard deviation\n",
    "def boostrap_std(sample, sample_size, n_iterations):\n",
    "\tbootstrap_samples=np.random.choice(sample, size=(n_iterations,sample_size), replace=True)\n",
    "\tstd_array=np.std(bootstrap_samples, axis=1)\n",
    "\tdata_std=np.mean(std_array)\n",
    "\treturn data_std\n",
    "\n",
    "#Distance function from the Total within-cluster sum of sqaures to the arch. Used during the Elbow method \n",
    "def calc_distance(x1,y1,a,b,c):\n",
    "  d=abs(a*x1+b*y1+c)/((a*a+b*b)**0.5)\n",
    "  return d\n",
    "\t\n",
    "#permutation test: mean difference\tp-value\n",
    "def permutation_test(array1, array2, n_iterations):\n",
    "  results=[]\n",
    "  current_mean=np.mean(array1)\n",
    "  new_mean=np.mean(array2)\n",
    "  t_obs=new_mean-current_mean\n",
    "  concat = np.concatenate((array1, array2))\n",
    "  for i in range(n_iterations):    \n",
    "    perm = np.random.permutation(concat)\n",
    "    pcurrent= perm[:len(array1)]\n",
    "    pnew= perm[len(array1):]\n",
    "    t_perm=np.mean(pnew)-np.mean(pcurrent)\n",
    "    if t_perm>t_obs:\n",
    "      results.append(1)\n",
    "    else:\n",
    "      results.append(0)\n",
    "  p_value=np.sum(results)/n_iterations\n",
    "  return(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de492774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of numerical variables\n",
    "categorical=[\"Date\", \"Location\",\"WindGustDir\", \"WindDir9am\",\"WindDir3pm\",\"RainToday\"]\n",
    "x2_train.loc[:, ~x2_train.columns.isin(categorical)].astype(int).describe().transpose()\n",
    "numerical= x2_train.columns[~x2_train.columns.isin(categorical)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c99721",
   "metadata": {},
   "source": [
    "# Random Forest Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e389c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: RandomForestClassifier\n",
      "accuracy \n",
      "mean: 1.0000, lower: 0.9999, upper: 1.0000, std: 0.0001 \n",
      "f1: \n",
      "mean: 0.9999, lower: 0.9997, upper: 1.0000, std: 0.0002 \n",
      "g-mean \n",
      "mean: 0.9999, lower: 0.9997, upper: 1.0000, std: 0.0003 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We use the same folds from this point on\n",
    "k_fold = KFold(n_splits=10)\n",
    "#Empty lists to store results\n",
    "accuracy_base=[]\n",
    "f1_base=[]\n",
    "g_mean_base=[]\n",
    "metrics_base={\"accuracy\": accuracy_base, \"f1:\": f1_base, \"g-mean\": g_mean_base}\n",
    "\n",
    "#baselines\n",
    "rf_base=  RandomForestClassifier(500, random_state=21)\n",
    "clf=[rf_base]\n",
    "\n",
    "#10-fold cross validation trhough each baseline\n",
    "for c in clf:\n",
    "  c_name=c.__class__.__name__\n",
    "  print(\"Classifier: %s\" % c_name)\n",
    "  for train_indices, test_indices in k_fold.split(x2_train[numerical]):\n",
    "    scaler = StandardScaler()\n",
    "    #Train-validation split using best features\n",
    "    x2_train_f=x2_train[numerical].iloc[train_indices,:].copy()\n",
    "    x2_train_f=scaler.fit_transform(x2_train_f[:])\n",
    "    y2_train_f=y2_train.iloc[train_indices].copy()\n",
    "    x2_val_f=x2_train[numerical].iloc[test_indices,:].copy()\n",
    "    x2_val_f=scaler.transform(x2_val_f[:])\n",
    "    y2_val_f=y2_train.iloc[test_indices].copy()\n",
    "  \n",
    "    #training baseline\n",
    "    c.fit(x2_train_f, y2_train_f)\n",
    "       \n",
    "    #predicting class and computing metrics\n",
    "    y_hat=c.predict(x2_val_f)\n",
    "    f1_f=f1_score(y2_val_f,y_hat)\n",
    "    acc_f=accuracy_score(y2_val_f,y_hat)\n",
    "    pr_r=precision_score(y2_val_f,y_hat)\n",
    "    rec_f=recall_score(y2_val_f,y_hat)\n",
    "    gmean_f=(pr_r*rec_f)**(0.5)\n",
    "\n",
    "    #appending to results\n",
    "    accuracy_base.append([acc_f,c.__class__.__name__])\n",
    "    f1_base.append([f1_f,c.__class__.__name__])\n",
    "    g_mean_base.append([gmean_f,c.__class__.__name__])\n",
    "  \n",
    "  #printing results for each baseline\n",
    "  for m, v in metrics_base.items():\n",
    "    mean,l,u=boostrap_mean(np.array(v)[np.array(v)[:,1]==c_name][:,0].astype(float), \\\n",
    "                          len(np.array(v)[np.array(v)[:,1]==c_name][:,0].astype(float)),10000)\n",
    "    std=boostrap_std(np.array(v)[np.array(v)[:,1]==c_name][:,0].astype(float), \\\n",
    "                          len(np.array(v)[np.array(v)[:,1]==c_name][:,0].astype(float)),10000)\n",
    "    print(\"%s \\nmean: %0.4f, lower: %0.4f, upper: %0.4f, std: %0.4f \" % (m,mean, l,u,std))\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6cd61",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3d673949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Best k: 2\n",
      "Fold: 2\n",
      "Best k: 2\n",
      "Fold: 3\n",
      "Best k: 2\n",
      "Fold: 4\n",
      "Best k: 2\n",
      "Fold: 5\n",
      "Best k: 2\n",
      "Fold: 6\n",
      "Best k: 2\n",
      "Fold: 7\n",
      "Best k: 2\n",
      "Fold: 8\n",
      "Best k: 2\n",
      "Fold: 9\n",
      "Best k: 2\n",
      "Fold: 10\n",
      "Best k: 2\n",
      "\n",
      "\n",
      "accuracy \n",
      "mean: 1.0000, lower: 0.9999, upper: 1.0000, std: 0.0001 \n",
      "f1: \n",
      "mean: 0.9999, lower: 0.9997, upper: 1.0000, std: 0.0002 \n",
      "g-mean \n",
      "mean: 0.9999, lower: 0.9997, upper: 1.0000, std: 0.0003 \n"
     ]
    }
   ],
   "source": [
    "#defining maximum number of clusters\n",
    "kmax=5\n",
    "#creating empty lists to store results\n",
    "accuracy_cluster=[]\n",
    "f1_cluster=[]\n",
    "g_mean_cluster=[]\n",
    "metrics_cluster={\"accuracy\": accuracy_cluster, \"f1:\": f1_cluster, \"g-mean\": g_mean_cluster}\n",
    "\n",
    "#10-fold CV for cluster-based method\n",
    "i=1 #iterator variable\n",
    "for train_indices, test_indices in k_fold.split(x2_train):\n",
    "  print(\"Fold: %s\" % i) #control message\n",
    "  #Training-validation split\n",
    "  x2_train_f=x2_train[numerical].iloc[train_indices].copy()\n",
    "  x2_train_f[:]=scaler.fit_transform(x2_train_f[:])\n",
    "  y2_train_f=y2_train.iloc[train_indices].copy()\n",
    "  x2_val_f=x2_train[numerical].iloc[test_indices].copy()\n",
    "  x2_val_f[:]=scaler.transform(x2_val_f[:])\n",
    "  y2_val_f=y2_train.iloc[test_indices].copy()\n",
    "\n",
    "  #####################################\n",
    "  #Optimal number of clusters selection\n",
    "  #####################################\n",
    "  k_optimal=[]  #empty list to store results from each method\n",
    "  wss=[]  #empty list for sum of squares\n",
    "  silhouette_avg=[] #empty list for silhouette score\n",
    "\n",
    "  #compute 'kmax' different clusters\n",
    "  for k in range(1,kmax+1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=21)  #defining algorithm\n",
    "    cluster=kmeans.fit_predict(x2_train_f)  #fit kmeans\n",
    "    #append silhouette score >1 as for k=1 it is not defined\n",
    "    if k!=1:\n",
    "      sil=silhouette_score(x2_train_f, cluster)\n",
    "      silhouette_avg.append(sil)\n",
    "    #append sum of squares\n",
    "    wss.append(kmeans.inertia_)\n",
    "\n",
    "  ################\n",
    "  #Elbow Method\n",
    "  ################\n",
    "  #Rescale sum of squares\n",
    "#   wss=(wss-min(wss))/(max(wss)-min(wss))\n",
    "\n",
    "  #defining variables to compute distance from sum of squares curve\n",
    "  #to the arch connecting both extremes (see plot below)\n",
    "  a=wss[0]-wss[-1]\n",
    "  b=list(range(1,kmax+1))[-1]-list(range(1,kmax+1))[0]\n",
    "  c1=list(range(1,kmax+1))[0]*wss[-1]\n",
    "  c2=list(range(1,kmax+1))[-1]*wss[0]\n",
    "  c=c1-c2\n",
    "\n",
    "  #computing distance from sum of squares curve to the arch\n",
    "  distance=[]\n",
    "  for k in range(kmax):\n",
    "    distance.append(calc_distance(list(range(1,kmax+1))[k],wss[k],a,b,c))\n",
    "  #appending optimal K based on the elbow method\n",
    "  k_optimal.append(list(range(1,kmax+1))[np.argmax(distance)])\n",
    "\n",
    " ###################\n",
    "  #Silhouette Method \n",
    "  ################## \n",
    "  #appending optimal K as that with the maximum silhouette score\n",
    "  k_optimal.append(list(range(2,kmax+1))[np.argmax(silhouette_avg)])\n",
    "\n",
    "  #choosing the max between the two methods (as it yields better results\n",
    "  #than selecting the min)\n",
    "  k_optimal=max(k_optimal)\n",
    "  print(\"Best k: %s\" % k_optimal) #print optimal K\n",
    "\n",
    "  ################\n",
    "  #Learning stage\n",
    "  ################\n",
    "  #defining kmeans algorithm\n",
    "  kmeans = KMeans(n_clusters=k_optimal, random_state=21)\n",
    "  cluster=kmeans.fit_predict(x2_train_f)   #fitting kmeans algorithm\n",
    "\n",
    "  #checking whether all clusters have instances from the same class\n",
    "  i2=0  #iterator\n",
    "  rf={} #empty dictionary to store Random Forest classifieres\n",
    "  #iterating through the clusters\n",
    "  for k in range(k_optimal):\n",
    "    #checking if cluster is 'complete' (instances from same class)\n",
    "    if (y2_train_f[cluster==k].unique().shape[0]==1) is False:\n",
    "      #defining classifier and storing it in the dictionary\n",
    "      rf['rf_{0}'.format(k)] = RandomForestClassifier(500, random_state=21)\n",
    "      #training classifier\n",
    "      rf[list(rf.keys())[i2]].fit(x2_train_f[cluster==k],y2_train_f[cluster==k])\n",
    "      i2+=1 #increasing iterator\n",
    "\n",
    "  ##################\n",
    "  #predicting Stage\n",
    "  ##################\n",
    "  #fitting kmeans on validation set previoulsy trained using training set\n",
    "  y_hat=kmeans.predict(x2_val_f)\n",
    "  i3=0  #iterator\n",
    "  #Loop through clusters to check if it has instances from the same class\n",
    "  for k in range(k_optimal):\n",
    "    if (y2_train_f[cluster==k].unique().shape[0]==1) is True:\n",
    "      #if it is complete, assign the corresponding class to the validation instances in that cluster\n",
    "      y_hat[y_hat==k]=np.mean(y2_train_f[cluster==k])\n",
    "    else:\n",
    "      #if not use the pre-trained classifier to predict the class\n",
    "      y_hat[y_hat==k]=rf[list(rf.keys())[i3]].predict(x2_val_f[y_hat==k])\n",
    "      i3+=1 #increase iterator\n",
    "  \n",
    "  #computing metrics\n",
    "  f1_f=f1_score(y2_val_f,y_hat)\n",
    "  acc_f=accuracy_score(y2_val_f,y_hat)\n",
    "  pr_r=precision_score(y2_val_f,y_hat)\n",
    "  rec_f=recall_score(y2_val_f,y_hat)\n",
    "  gmean_f=(pr_r*rec_f)**(0.5)\n",
    "\n",
    "  #appending metrics to lists\n",
    "  accuracy_cluster.append([acc_f,i])\n",
    "  f1_cluster.append([f1_f,i])\n",
    "  g_mean_cluster.append([gmean_f,i])\n",
    "  i+=1 \n",
    "\n",
    "print(\"\\n\") \n",
    "#printing results\n",
    "for m, v in metrics_cluster.items():\n",
    "  mean,l,u=boostrap_mean(np.array(v)[:,0].astype(float), \\\n",
    "                        len(np.array(v)[:,0].astype(float)),10000)\n",
    "  std=boostrap_std(np.array(v)[:,0].astype(float), \\\n",
    "                        len(np.array(v)[:,0].astype(float)),10000)\n",
    "  print(\"%s \\nmean: %0.4f, lower: %0.4f, upper: %0.4f, std: %0.4f \" % (m,mean, l,u,std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8556ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score Mean Difference: Cluster-RF vs RandomForestClassifier:\n",
      "Difference: 0.0000, p-value: 0.2400\n",
      "G-mean Score Mean Difference: Cluster-RF vs RandomForestClassifier:\n",
      "Difference: 0.0000, p-value: 0.2590\n",
      "Accuracy Mean Difference: Cluster-RF vs RandomForestClassifier:\n",
      "Difference: 0.0000, p-value: 0.2040\n",
      "Best k: 2\n",
      "Test accuracy: 1.0000\n",
      "Test f1:: 1.0000\n",
      "Test g-mean: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 4. Permutation Test\n",
    "\n",
    "### A) F1 Score\n",
    "\n",
    "base=[\"RandomForestClassifier\"] #lists with baselines\n",
    "\n",
    "#compare our method against baselines\n",
    "for b in base:\n",
    "  print(\"F1 Score Mean Difference: Cluster-RF vs %s:\" % b)  #message\n",
    "  cluster=np.array(f1_cluster)[:,0] #getting results from our method\n",
    "  base=np.array(f1_base)[np.array(f1_base)[:,1]==b][:,0].astype(float) #baselines results\n",
    "  diff=np.mean(cluster)-np.mean(base) #computing mean difference\n",
    "  p_value=permutation_test(base,cluster,1000) #estimating p-value\n",
    "  print(\"Difference: %0.4f, p-value: %0.4f\" % (diff, p_value))  #printing p-value\n",
    "\n",
    "### B) G-mean \n",
    "\n",
    "base=[\"RandomForestClassifier\"]#lists with baselines\n",
    "\n",
    "#compare our method against baselines\n",
    "for b in base:\n",
    "  print(\"G-mean Score Mean Difference: Cluster-RF vs %s:\" % b)  #message\n",
    "  cluster=np.array(g_mean_cluster)[:,0] #getting results from our method\n",
    "  base=np.array(g_mean_base)[np.array(g_mean_base)[:,1]==b][:,0].astype(float)  #baselines results\n",
    "  diff=np.mean(cluster)-np.mean(base) #computing mean difference\n",
    "  p_value=permutation_test(base,cluster,1000) #estimating p-value\n",
    "  print(\"Difference: %0.4f, p-value: %0.4f\" % (diff, p_value))  #printing p-value\n",
    "\n",
    "### C) Accuracy\n",
    "\n",
    "base=[\"RandomForestClassifier\"]  #lists with baselines\n",
    "\n",
    "#compare our method against baselines\n",
    "for b in base:\n",
    "  print(\"Accuracy Mean Difference: Cluster-RF vs %s:\" % b)  #message\n",
    "  cluster=np.array(accuracy_cluster)[:,0]  #getting results from our method\n",
    "  base=np.array(accuracy_base)[np.array(accuracy_base)[:,1]==b][:,0].astype(float)  #baselines results\n",
    "  diff=np.mean(cluster)-np.mean(base) #computing mean difference\n",
    "  p_value=permutation_test(base,cluster,1000) #estimating p-value\n",
    "  print(\"Difference: %0.4f, p-value: %0.4f\" % (diff, p_value))  #printing p-value\n",
    "\n",
    "#converting results to data frames to plot them\n",
    "\n",
    "m_t=pd.DataFrame([])  #empty data frame\n",
    "#iterating through cluster-based method results\n",
    "for m, v in metrics_cluster.items():\n",
    "  if m!=\"accuracy\":\n",
    "    m_i=pd.DataFrame(v) #converting results to dataframe\n",
    "    m_i[\"metric\"]=m #adding results\n",
    "    m_i.iloc[:,1]=\"cluster\" #indicator column\n",
    "    m_t=pd.concat([m_t,m_i], ignore_index=True) #appending to empy dataframe\n",
    "\n",
    "#iterating through baseline results\n",
    "for m, v in metrics_base.items():\n",
    "  if m!=\"accuracy\":\n",
    "    m_i=pd.DataFrame(v) #converting results to dataframe\n",
    "    m_i[\"metric\"]=m  #adding results\n",
    "    m_t=pd.concat([m_t,m_i], ignore_index=True) #appending to empy dataframe\n",
    " \n",
    "\n",
    "# #Plotting results\n",
    "plot=sns.boxplot(m_t.iloc[:,2],m_t.iloc[:,0], hue=m_t.iloc[:,1],\n",
    "                 data=m_t)\n",
    "\n",
    "handles, _ = plot.get_legend_handles_labels()\n",
    "\n",
    "plt.legend( title=\"\", fontsize='small', fancybox=True)\n",
    "plot.legend(handles, [\"Kmeans-RF\", \"DT\", \"RF\"],ncol=3,loc='upper left')\n",
    "plot.set(xlabel='Metric', ylabel='Value') #labeling axis\n",
    "plot.set_xticks(range(m_t.iloc[:,2].unique().shape[0])) # <--- set the ticks first\n",
    "plot.set_xticklabels([\"F1 Score\", \"G-mean\"])\n",
    "#plot.set_yscale('log')\n",
    "#plot.yaxis.set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "#plot.set_yticks([0.5,0.55, 0.6])\n",
    "plot.set_ylim((.5,.65))\n",
    "plt.savefig(\"weather_box.png\", bbox_inches='tight') #saving figure\n",
    "\n",
    "### 5. Test Set Results\n",
    "\n",
    "\n",
    "#####################################\n",
    "#Optimal number of clusters selection\n",
    "#####################################\n",
    "k_optimal=[]  #empty list to store results from each method\n",
    "wss=[]  #empty list for sum of squares\n",
    "silhouette_avg=[] #empty list for silhouette score\n",
    "#Iterating thorugh the maximum number of clusters to be tested\n",
    "for k in range(1,kmax+1):\n",
    "  kmeans = KMeans(n_clusters=k, random_state=21)\n",
    "  cluster=kmeans.fit_predict(x2_train[numerical])\n",
    "  if k!=1:\n",
    "      sil=silhouette_score(x2_train[numerical], cluster)\n",
    "      silhouette_avg.append(sil)\n",
    "  wss.append(kmeans.inertia_)\n",
    "\n",
    "#Elbow Method\n",
    "#Rescale sum of squares\n",
    "# wss=(wss-min(wss))/(max(wss)-min(wss))\n",
    "#defining variables to compute distance from sum of squares curve\n",
    "#to the arch connecting both extremes (see plot below)\n",
    "a=wss[0]-wss[-1]\n",
    "b=list(range(1,kmax+1))[-1]-list(range(1,kmax+1))[0]\n",
    "c1=list(range(1,kmax+1))[0]*wss[-1]\n",
    "c2=list(range(1,kmax+1))[-1]*wss[0]\n",
    "c=c1-c2\n",
    "\n",
    "#computing distance from sum of squares curve to the arch\n",
    "distance=[]\n",
    "for k in range(kmax):\n",
    "  distance.append(calc_distance(list(range(1,kmax+1))[k],wss[k],a,b,c))\n",
    "#appending optimal K based on the elbow method\n",
    "k_optimal.append(list(range(1,kmax+1))[np.argmax(distance)])\n",
    "\n",
    "###################\n",
    "#Silhouette Method \n",
    "################## \n",
    "#appending optimal K as that with the maximum silhouette score\n",
    "k_optimal.append(list(range(2,kmax+1))[np.argmax(silhouette_avg)])\n",
    "\n",
    "#choosing the max between the two methods (as it yields better results\n",
    "#than selecting the min)\n",
    "k_optimal=max(k_optimal)\n",
    "print(\"Best k: %s\" % k_optimal) #print optimal K\n",
    "\n",
    "################\n",
    "#Learning stage\n",
    "################\n",
    "#defining kmeans algorithm\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=21)\n",
    "cluster=kmeans.fit_predict(x2_train[numerical])\n",
    "\n",
    "#checking whether all clusters have instances from the same class\n",
    "i2=0  #iterator\n",
    "rf={} #empty dictionary to store Random Forest classifieres\n",
    "#iterating through the clusters\n",
    "for k in range(k_optimal):\n",
    "  #checking if cluster is 'complete' (instances from same class)\n",
    "  if (y2_train[cluster==k].unique().shape[0]==1) is False:\n",
    "    #defining classifier and storing it in the dictionary\n",
    "    rf['rf_{0}'.format(k)] = RandomForestClassifier(500, random_state=21)\n",
    "    #training classifier\n",
    "    rf[list(rf.keys())[i2]].fit(x2_train[numerical].iloc[cluster==k],y2_train[cluster==k])\n",
    "    i2+=1\n",
    "\n",
    "##################\n",
    "#predicting Stage\n",
    "##################\n",
    "#fitting kmeans on validation set previoulsy trained using training set\n",
    "y_hat=kmeans.predict(x2_test[numerical])\n",
    "i3=0#iterator\n",
    "#Loop through clusters to check if it has instances from the same class\n",
    "for k in range(k_optimal):\n",
    "  #checking if cluster is 'complete'\n",
    "  if (y2_train[cluster==k].unique().shape[0]==1) is True:\n",
    "    #if it is complete, assign the corresponding class to the validation instances in that cluster\n",
    "    y_hat[y_hat==k]=np.mean(y2_train[cluster==k])\n",
    "  else:\n",
    "    #if not use the pre-trained classifier to predict the class\n",
    "    y_hat[y_hat==k]=rf[list(rf.keys())[i3]].predict(x2_test[numerical].iloc[y_hat==k])\n",
    "    i3+=1\n",
    "\n",
    "#computing metrics\n",
    "f1_t=f1_score(y2_test,y_hat)\n",
    "acc_t=accuracy_score(y2_test,y_hat)\n",
    "pr_t=precision_score(y2_test,y_hat)\n",
    "rec_t=recall_score(y2_test,y_hat)\n",
    "gmean_t=(pr_t*rec_t)**(0.5)\n",
    "\n",
    "#pritning results\n",
    "metrics_test={\"accuracy\": acc_t, \"f1:\": f1_t, \"g-mean\": gmean_t}\n",
    "for m, v in metrics_test.items():\n",
    "  print(\"Test %s: %0.4f\" % (m,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a14d30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 2\n",
      "Test accuracy: 1.0000\n",
      "Test f1:: 1.0000\n",
      "Test g-mean: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "#Optimal number of clusters selection\n",
    "#####################################\n",
    "k_optimal=[]  #empty list to store results from each method\n",
    "wss=[]  #empty list for sum of squares\n",
    "silhouette_avg=[] #empty list for silhouette score\n",
    "#Iterating thorugh the maximum number of clusters to be tested\n",
    "for k in range(1,kmax+1):\n",
    "  kmeans = KMeans(n_clusters=k, random_state=21)\n",
    "  cluster=kmeans.fit_predict(x2_train[numerical])\n",
    "  if k!=1:\n",
    "      sil=silhouette_score(x2_train[numerical], cluster)\n",
    "      silhouette_avg.append(sil)\n",
    "  wss.append(kmeans.inertia_)\n",
    "\n",
    "#Elbow Method\n",
    "#Rescale sum of squares\n",
    "# wss=(wss-min(wss))/(max(wss)-min(wss))\n",
    "#defining variables to compute distance from sum of squares curve\n",
    "#to the arch connecting both extremes (see plot below)\n",
    "a=wss[0]-wss[-1]\n",
    "b=list(range(1,kmax+1))[-1]-list(range(1,kmax+1))[0]\n",
    "c1=list(range(1,kmax+1))[0]*wss[-1]\n",
    "c2=list(range(1,kmax+1))[-1]*wss[0]\n",
    "c=c1-c2\n",
    "\n",
    "#computing distance from sum of squares curve to the arch\n",
    "distance=[]\n",
    "for k in range(kmax):\n",
    "  distance.append(calc_distance(list(range(1,kmax+1))[k],wss[k],a,b,c))\n",
    "#appending optimal K based on the elbow method\n",
    "k_optimal.append(list(range(1,kmax+1))[np.argmax(distance)])\n",
    "\n",
    "###################\n",
    "#Silhouette Method \n",
    "################## \n",
    "#appending optimal K as that with the maximum silhouette score\n",
    "k_optimal.append(list(range(2,kmax+1))[np.argmax(silhouette_avg)])\n",
    "\n",
    "#choosing the max between the two methods (as it yields better results\n",
    "#than selecting the min)\n",
    "k_optimal=max(k_optimal)\n",
    "print(\"Best k: %s\" % k_optimal) #print optimal K\n",
    "\n",
    "################\n",
    "#Learning stage\n",
    "################\n",
    "#defining kmeans algorithm\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=21)\n",
    "cluster=kmeans.fit_predict(x2_train[numerical])\n",
    "\n",
    "#checking whether all clusters have instances from the same class\n",
    "i2=0  #iterator\n",
    "rf={} #empty dictionary to store Random Forest classifieres\n",
    "#iterating through the clusters\n",
    "for k in range(k_optimal):\n",
    "  #checking if cluster is 'complete' (instances from same class)\n",
    "  if (y2_train[cluster==k].unique().shape[0]==1) is False:\n",
    "    #defining classifier and storing it in the dictionary\n",
    "    rf['rf_{0}'.format(k)] = RandomForestClassifier(500, random_state=21)\n",
    "    #training classifier\n",
    "    rf[list(rf.keys())[i2]].fit(x2_train[numerical].iloc[cluster==k],y2_train[cluster==k])\n",
    "    i2+=1\n",
    "\n",
    "##################\n",
    "#predicting Stage\n",
    "##################\n",
    "#fitting kmeans on validation set previoulsy trained using training set\n",
    "y_hat=kmeans.predict(x2_test[numerical])\n",
    "i3=0#iterator\n",
    "#Loop through clusters to check if it has instances from the same class\n",
    "for k in range(k_optimal):\n",
    "  #checking if cluster is 'complete'\n",
    "  if (y2_train[cluster==k].unique().shape[0]==1) is True:\n",
    "    #if it is complete, assign the corresponding class to the validation instances in that cluster\n",
    "    y_hat[y_hat==k]=np.mean(y2_train[cluster==k])\n",
    "  else:\n",
    "    #if not use the pre-trained classifier to predict the class\n",
    "    y_hat[y_hat==k]=rf[list(rf.keys())[i3]].predict(x2_test[numerical].iloc[y_hat==k])\n",
    "    i3+=1\n",
    "\n",
    "#computing metrics\n",
    "f1_t=f1_score(y2_test,y_hat)\n",
    "acc_t=accuracy_score(y2_test,y_hat)\n",
    "pr_t=precision_score(y2_test,y_hat)\n",
    "rec_t=recall_score(y2_test,y_hat)\n",
    "gmean_t=(pr_t*rec_t)**(0.5)\n",
    "\n",
    "#pritning results\n",
    "metrics_test={\"accuracy\": acc_t, \"f1:\": f1_t, \"g-mean\": gmean_t}\n",
    "for m, v in metrics_test.items():\n",
    "  print(\"Test %s: %0.4f\" % (m,v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
